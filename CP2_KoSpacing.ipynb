{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwon-n/cp2/blob/hyewon/CP2_KoSpacing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J2kElC3mkwU"
      },
      "source": [
        "# 띄어쓰기 검사 모델 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEqSiuM1m3a5"
      },
      "source": [
        "## 필요한 패키지 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2taSuFyo27r",
        "outputId": "ae38c497-1a37-4794-d22e-c64784df7c2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsMpUNHmmvr3"
      },
      "outputs": [],
      "source": [
        "pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf3K_wAi4aho"
      },
      "outputs": [],
      "source": [
        "pip install git+https://github.com/ssut/py-hanspell.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPw2mz7O6G7l"
      },
      "outputs": [],
      "source": [
        "pip install soynlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0W6cVCW7KDX"
      },
      "outputs": [],
      "source": [
        "pip install customized_konlpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "metadata": {
        "id": "f1Xqd_13Uoy-",
        "outputId": "7eac093d-b2fe-4a73-8d2d-e855f4df3be3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-stq1md9a\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-stq1md9a\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.21.19-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting gluonnlp>=0.6.0\n",
            "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
            "\u001b[K     |████████████████████████████████| 344 kB 62.6 MB/s \n",
            "\u001b[?25hCollecting mxnet>=1.4.0\n",
            "  Downloading mxnet-1.9.0-py3-none-manylinux2014_x86_64.whl (47.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.3 MB 58 kB/s \n",
            "\u001b[?25hCollecting onnxruntime==1.8.0\n",
            "  Downloading onnxruntime-1.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 40.2 MB/s \n",
            "\u001b[?25hCollecting sentencepiece>=0.1.6\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 32.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (1.10.0+cu111)\n",
            "Collecting transformers>=4.8.1\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 38.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (1.19.5)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (1.12)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (3.17.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (0.29.28)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (21.3)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->kobert==0.2.3) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (2021.10.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->kobert==0.2.3) (3.7.4.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 50.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (4.11.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 50.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (4.63.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp>=0.6.0->kobert==0.2.3) (3.0.7)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting botocore<1.25.0,>=1.24.19\n",
            "  Downloading botocore-1.24.19-py3-none-any.whl (8.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 15.1 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.19->boto3->kobert==0.2.3) (2.8.2)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 73.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.19->boto3->kobert==0.2.3) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.8.1->kobert==0.2.3) (3.7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (7.1.2)\n",
            "Building wheels for collected packages: kobert, gluonnlp\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15674 sha256=84a59e0e5d762516b29d0b83dd9d47f3dc8a93f79999af25d20ccdf01680be26\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zg14yzq9/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595737 sha256=d0b2e1e410e14b086aa9252cc571cff8ce5b098d3cd0ac220e005d21d0d34cb9\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n",
            "Successfully built kobert gluonnlp\n",
            "Installing collected packages: urllib3, jmespath, pyyaml, botocore, tokenizers, sacremoses, s3transfer, huggingface-hub, graphviz, transformers, sentencepiece, onnxruntime, mxnet, gluonnlp, boto3, kobert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.21.19 botocore-1.24.19 gluonnlp-0.10.0 graphviz-0.8.4 huggingface-hub-0.4.0 jmespath-0.10.0 kobert-0.2.3 mxnet-1.9.0 onnxruntime-1.8.0 pyyaml-6.0 s3transfer-0.5.2 sacremoses-0.0.47 sentencepiece-0.1.96 tokenizers-0.11.6 transformers-4.17.0 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "cRDK4zWk144r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5Cr0ERXqpCEm"
      },
      "outputs": [],
      "source": [
        "\n",
        "from hanspell import spell_checker\n",
        "\n",
        "import urllib.request\n",
        "from soynlp import DoublespaceLineCorpus\n",
        "from soynlp.word import WordExtractor\n",
        "from soynlp.tokenizer import LTokenizer\n",
        "from soynlp.tokenizer import MaxScoreTokenizer\n",
        "from soynlp.normalizer import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3CdzqLFpADv"
      },
      "source": [
        "## 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dBPvqzqpG_w",
        "outputId": "af640dfd-305a-4b45-e58e-d989d909c66c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12600\n"
          ]
        }
      ],
      "source": [
        "root = '/content/drive/MyDrive/news_class9x1400/'\n",
        "categories = os.listdir(root)\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for cat in categories:\n",
        "  files = os.listdir(root + cat)\n",
        "  for i, f in enumerate(files):\n",
        "    fname = root + cat + '/' + f\n",
        "    file = open(fname, 'r', encoding='utf-8')\n",
        "    strings = file.read()\n",
        "    dataset.append([strings])\n",
        "    file.close()\n",
        "\n",
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(dataset)\n",
        "mini_dataset = dataset[:1000]"
      ],
      "metadata": {
        "id": "k6zv1WC7WLWG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMI63w5l_099",
        "outputId": "feba5e21-3c96-4ce6-b59e-2ca7cd0de14d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['찜통더위 물렀거라!\\n낮잠이 보약\\n산에서 낮잠 자는 사람들이 왜 이렇게 많아?\\n그러게 말이야, 낮잠은 방안이나 정자에서 자야 되는 거 아냐?\\n낮잠 자는 장소가 따로 정해져 있나?\\n졸리면 자는 거지.\\n일행들이 한 마디씩 한다.\\n서울 관악구 신림동 관악산에서 능선으로 이어진 삼성산 등산길에는 낮잠 자는 사람들이 많았다.\\n전에도 이 산을 많이 올랐지만 이런 풍경은 처음이었다.\\n부부끼리, 친구끼리, 또는 홀로 나무그늘에 돗자리를 펴고 들어 누워 낮잠 자는 사람들은 자세도 다양했다.\\n64회 광복절이자 주말인 15일 친구들 몇 사람과 함께 관악산 등산에 나섰다.\\n일행들은 서울대학교 정문 쪽 등산로 입구 광장 시계탑 밑에서 만났다.\\n친구들은 이번에도 관악산이 아닌 삼성산으로 올라가잔다.\\n이유는 날씨가 너무 무덥다는 것이었다.\\n주차장에서 곧장 아파트 옆 골목으로 빠져 등산로로 나섰다.\\n그런데 무더위가 정말 장난이 아니었다.\\n서울지방의 기온이 섭씨 33도가 넘을 것이라는 일기예보가 실감나는 날씨였다.\\n땡볕은 그렇다 해도 나무그늘이 드리워진 산길도 푹푹 찌는 열기가 온몸을 휘감고 있었다.\\n히야! 이거 더워 죽겠네, 이쪽으로 오르길 잘했지, 이런 날씨에 저쪽 관악산이었으면 어쩔 뻔 했어?\\n잠깐 쉬어가지?\\n경사가 완만한 길인데도 얼마 올라가지 못하고 땀으로 뒤범벅이 된 얼굴로 모두들 길가에 주저앉는다.\\n잠시 쉬었다가 다시 오르기 시작했다.\\n그런데 약간 가파른 길을 올라도 숨이 헉헉 턱에 찬다.\\n너무 무더운 날씨 때문이었다.\\n그렇게 허위허위 청소년야영장이 있는 능선길에 올라섰다.\\n그런데 능선길에 올라서자 사정없이 내려 쏟아지는 햇볕이 그야말로 불덩이다.\\n햇볕을 피해 나무그늘 밑으로 숨어들자 곳곳에 낮잠 자는 사람들이 눈에 띄기 시작했다.\\n부부로 보이는 어떤 커플은 나무그늘이 아니라 땡볕에서 깊이 잠든 모습이다.\\n그늘 밑에서 잠들었다가 그늘이 옮겨진 것도 모른 채 계속 자고 있었기 때문일 것이다.\\n조금 더 걸어가자 이번엔 역시 부부로 보이는 사람들이 검정 우산을 머리 위에 펴놓은 채 잠들어 있는 모습이다.\\n우산으로 햇빛과 얼굴을 가린 모습이 재치 있고 재미있어 보인다.\\n능선 나무그늘 밑에 모여 점심을 먹기로 했다.\\n마침 실바람이 솔솔 불어온다.\\n약한 바람이지만 시원하기 짝이 없었다.\\n모두들 도시락을 펴놓고 점심을 먹었다.\\n정상주 한 잔씩에 준비해온 과일까지 곁들이니 제법 푸짐한 점심이다.\\n우리들도 여기 누워 낮잠 한숨 자고 갔으면 좋겠는 걸.\\n잠이 보약이라잖아.\\n이럴 땐 낮잠 한숨이 더위를 이기는 최고의 보약이야.\\n난 잠깐만 자야겠어, 도저히 참을 수가 없는 걸.\\n무더운 날씨에 힘들게 올라와 술 한잔을 곁들인 점심을 먹고 나니 낮잠 한숨이 간절하다.\\n일행 한 사람이 졸린다며 슬그머니 드러누웠나 했는데 어느새 코를 살살 곤다.\\n참 달콤하고 편안한 잠이다.\\n그러나 다른 사람들은 감히 낮잠 잘 생각을 하지 않는다.\\n아무리 무덥기로 산 속에서 낮잠을 잘 수가 있나?\\n난 드러누워도 잠이 올 것 같지 않은데.\\n뭐, 어때?\\n다른 사람들은 저렇게 잘 자는데.\\n잠자는 친구는 정말 천하태평으로 잘도 잔다.\\n이따금 개미들이 잠자는 친구의 몸 위를 기어 다닌다.\\n맨몸인 팔과 다리 위를 개미가 기어 다닐 때는 느낌이 좋지 않은지 몸을 꿈틀거리기도 하고, 손으로 잡으려는 몸짓도 하여 옆에 앉아 있는 다른 친구들이 대신 쫓아 주기도 했다.\\n어, 잘 잤다, 몸도 마음도 가뿐한 걸, 찜통더위엔 역시 낮잠이 최고의 보약이라니까.\\n그렇게 20여분 동안 낮잠을 잔 친구가 부스스 일어나며 시원한 기지개를 켠다.\\n다른 친구가 이제 그만 가자며 몸을 흔들어 깨웠기 때문이다.\\n잠들었던 친구는 조금 아쉬운 표정이었지만 20여 분의 산속 낮잠이 피로를 말끔하게 씻어주었다고 좋아한다.\\n능선길과 평평한 나무그늘 밑에는 여기저기 꽤 많은 사람들이 잠들어 있었다.\\n수건으로 얼굴을 덮고 잠들어 있는 여성등산객도 보이고, 나무와 나무 사이에 매달아 놓은 그네 위에서 잠든 남자도 보인다.\\n길가 숲속 정자 위에서는 아주머니 두 사람이 시원하게 잠들어 있는 모습이다.\\n골짜기로 내려오자 개울가에는 사람들이 더욱 많았다.\\n물가에 모여 앉아 음식을 나누어먹는 사람들과 물속에서 깔깔거리는 사람들, 모두 올여름 들어 가장 무덥다는 찜통더위를 이겨내는 방법이었다.\\n넓고 수량이 많은 골짜기 아래쪽은 물놀이 장소로 만들어 놓은 곳이었다.\\n이곳에는 수많은 어린이들과 근처 지역에서 모여든 사람들이 와글와글 했다.\\n이들도 흐르는 물속에서 물놀이를 하며 역시 무더위를 식히는 모습이었다.\\n개울 한 곳에는 십대로 보이는 여학생들 여섯 명과 초등학생으로 보이는 여자 어린이가 개울물 속에 둘러앉아 이야기꽃을 피우고 있는 모습이다.\\n개울물 속 7공주의 망중한이었다.\\n조금 더 내려오자 이번에는 개울가 땡볕 아래 앉아 독서삼매경에 빠진 중년 남성 한 사람의 모습이 눈길을 붙잡는다.\\n이날 주말의 찜통 무더위를 피해 시원한 바다로 나간 사람들도 많았을 것이다.\\n그러나 서울시내에서 가까운 삼성산과 관악산 사이 골짜기를 찾은 사람들은 낮잠과 물놀이, 그리고 독서 등 저마다의 방법으로 더위를 이겨내고 있었다.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "mini_dataset[241]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAMmcAbI__hf"
      },
      "source": [
        "## 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuOqE2rFCdh5",
        "outputId": "07266c52-d50b-463d-8d95-f8bc3f0fa562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kss in /usr/local/lib/python3.7/dist-packages (3.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from kss) (2019.12.20)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (from kss) (1.7.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install kss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table = str.maketrans({\n",
        "    '\\'': '',\n",
        "    '\\\"': '',\n",
        "    '★': '',\n",
        "    '▲': '',\n",
        "    '┌': '',\n",
        "    '├': '',\n",
        "    '│': '',\n",
        "    '└': '',\n",
        "    '→': ''\n",
        "\n",
        "})\n",
        "test = '\"안녕\"하세★요? 좋은 아▲침입┌니├다. 오│늘도 힘내└세요→!'\n",
        "test = test.translate(table)\n",
        "\n",
        "test"
      ],
      "metadata": {
        "id": "qdHAtAEy5VSa",
        "outputId": "ad99ed98-c9eb-4f64-eb08-0610786c36ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'안녕하세요? 좋은 아침입니다. 오늘도 힘내세요!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqZ4DX-5_3zR"
      },
      "outputs": [],
      "source": [
        "import kss\n",
        "import random\n",
        "\n",
        "process_dataset = []\n",
        "\n",
        "for sentences in mini_dataset:\n",
        "  sentences = sentences[0]\n",
        "  sentences = sentences.translate(table)\n",
        "  sentences = sentences.split(\"\\n\")\n",
        "\n",
        "  for sentence in sentences:\n",
        "    process_dataset.extend(kss.split_sentences(sentence))\n",
        "\n",
        "\n",
        "len(process_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4h 36m \n",
        "\n",
        "len(process_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXmDsOkbuiNI",
        "outputId": "83bf5205-017a-4342-c438-ef556efdeb03"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22003"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(process_dataset, test_size = 0.2, random_state  = 42)\n",
        "print(len(train), len(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9ISMwvSmTlo",
        "outputId": "ae0777cf-8a87-4e17-ba74-d397a97cab4f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17602 4401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_datafile(file_name, dataset):\n",
        "  with open(\n",
        "      os.path.join(root, file_name), mode = 'w', encoding='utf-8'\n",
        "  ) as f:\n",
        "    for data in dataset:\n",
        "      f.write(data)\n",
        "\n",
        "save_datafile('train.txt', train)\n",
        "save_datafile('test.txt', test)\n",
        "\n",
        "print('train, test dataset 저장 완료!')"
      ],
      "metadata": {
        "id": "PXPghQzzmp8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 띄어쓰기를 위한 클래스"
      ],
      "metadata": {
        "id": "PpWPjR0wTS7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "def load_slot_labels() -> List[str]:\n",
        "    \"\"\"tag label 종류 리턴\"\"\"\n",
        "    return [\"UNK\", \"PAD\", \"O\", \"B\", \"I\", \"E\", \"S\"]"
      ],
      "metadata": {
        "id": "yiZbUrjboyja"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, List, Tuple\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CorpusDataset(Dataset):\n",
        "  def __init__(self, data_path: str, transform: Callable[[List, List], Tuple]):\n",
        "    self.sentences = []\n",
        "    self.slot_labels = ['UNK', 'PAD', 'B', 'I']\n",
        "    self._load_data(data_path)\n",
        "    self.transform = transform\n",
        "\n",
        "  def _load_data(self, data_path: str):\n",
        "    with open(data_path, mode = 'r', encoding = 'utf-8') as f:\n",
        "      lines = f.readlines()\n",
        "      self.sentences = [line.split() for line in lines]\n",
        "\n",
        "  def _get_tags(self, sentences: List[str]) -> List[str]:\n",
        "    tags = []\n",
        "    for word in sentence:\n",
        "      for i in range(len(word)):\n",
        "        if i == 0:\n",
        "            tags.append(\"B\")\n",
        "        else:\n",
        "            tags.append(\"I\")\n",
        "    return tags\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sentences)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    sentence = \"\".join(self.sentences[idx])\n",
        "    sentence = [s for s in sentence]\n",
        "    tags = self._get_tags(self.sentences[idx])\n",
        "    tags = [self.slot_labels.index(t) for t in tags]\n",
        "\n",
        "    (\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "        token_type_ids,\n",
        "        slot_label_ids, \n",
        "    ) = self.transform(sentence, tags)\n",
        "\n",
        "    return input_ids, attention_mask, token_type_ids, slot_label_ids"
      ],
      "metadata": {
        "id": "LqZH3YA0nRA0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "tj7f1PABWx2F",
        "outputId": "03334449-a047-4e5a-9fa0-5d88467b5b02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tokenizers"
      ],
      "metadata": {
        "id": "5Odj4Jg7XKEO",
        "outputId": "7c41b064-fbec-4692-cc3f-694ff78fc66f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.11.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization_kobert\n"
      ],
      "metadata": {
        "id": "cRz2dZChaYF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team and Jangwon Park\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" Tokenization classes for KoBERT model \"\"\"\n",
        "\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import unicodedata\n",
        "from shutil import copyfile\n",
        "\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "VOCAB_FILES_NAMES = {\n",
        "    \"vocab_file\": \"tokenizer_78b3253a26.model\",\n",
        "    \"vocab_txt\": \"vocab.txt\",\n",
        "}\n",
        "\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\n",
        "    \"vocab_file\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\",\n",
        "    },\n",
        "    \"vocab_txt\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\",\n",
        "    },\n",
        "}\n",
        "\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
        "    \"monologg/kobert\": 512,\n",
        "    \"monologg/kobert-lm\": 512,\n",
        "    \"monologg/distilkobert\": 512,\n",
        "}\n",
        "\n",
        "PRETRAINED_INIT_CONFIGURATION = {\n",
        "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
        "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
        "    \"monologg/distilkobert\": {\"do_lower_case\": False},\n",
        "}\n",
        "\n",
        "SPIECE_UNDERLINE = \"▁\"\n",
        "\n",
        "\n",
        "class KoBertTokenizer(PreTrainedTokenizer):\n",
        "    \"\"\"\n",
        "    SentencePiece based tokenizer. Peculiarities:\n",
        "        - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n",
        "    \"\"\"\n",
        "\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_file,\n",
        "        vocab_txt,\n",
        "        do_lower_case=False,\n",
        "        remove_space=True,\n",
        "        keep_accents=False,\n",
        "        unk_token=\"[UNK]\",\n",
        "        sep_token=\"[SEP]\",\n",
        "        pad_token=\"[PAD]\",\n",
        "        cls_token=\"[CLS]\",\n",
        "        mask_token=\"[MASK]\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            unk_token=unk_token,\n",
        "            sep_token=sep_token,\n",
        "            pad_token=pad_token,\n",
        "            cls_token=cls_token,\n",
        "            mask_token=mask_token,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        # Build vocab\n",
        "        self.token2idx = dict()\n",
        "        self.idx2token = []\n",
        "        with open(vocab_txt, \"r\", encoding=\"utf-8\") as f:\n",
        "            for idx, token in enumerate(f):\n",
        "                token = token.strip()\n",
        "                self.token2idx[token] = idx\n",
        "                self.idx2token.append(token)\n",
        "\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\n",
        "                \"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                \"pip install sentencepiece\"\n",
        "            )\n",
        "\n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.remove_space = remove_space\n",
        "        self.keep_accents = keep_accents\n",
        "        self.vocab_file = vocab_file\n",
        "        self.vocab_txt = vocab_txt\n",
        "\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(vocab_file)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.idx2token)\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return dict(self.token2idx, **self.added_tokens_encoder)\n",
        "\n",
        "    def __getstate__(self):\n",
        "        state = self.__dict__.copy()\n",
        "        state[\"sp_model\"] = None\n",
        "        return state\n",
        "\n",
        "    def __setstate__(self, d):\n",
        "        self.__dict__ = d\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\n",
        "                \"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                \"pip install sentencepiece\"\n",
        "            )\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(self.vocab_file)\n",
        "\n",
        "    def preprocess_text(self, inputs):\n",
        "        if self.remove_space:\n",
        "            outputs = \" \".join(inputs.strip().split())\n",
        "        else:\n",
        "            outputs = inputs\n",
        "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
        "\n",
        "        if not self.keep_accents:\n",
        "            outputs = unicodedata.normalize(\"NFKD\", outputs)\n",
        "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
        "        if self.do_lower_case:\n",
        "            outputs = outputs.lower()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        \"\"\"Tokenize a string.\"\"\"\n",
        "        text = self.preprocess_text(text)\n",
        "        pieces = self.sp_model.encode(text, out_type=str)\n",
        "        new_pieces = []\n",
        "        for piece in pieces:\n",
        "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
        "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
        "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
        "                    if len(cur_pieces[0]) == 1:\n",
        "                        cur_pieces = cur_pieces[1:]\n",
        "                    else:\n",
        "                        cur_pieces[0] = cur_pieces[0][1:]\n",
        "                cur_pieces.append(piece[-1])\n",
        "                new_pieces.extend(cur_pieces)\n",
        "            else:\n",
        "                new_pieces.append(piece)\n",
        "\n",
        "        return new_pieces\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
        "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
        "\n",
        "    def _convert_id_to_token(self, index):\n",
        "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
        "        return self.idx2token[index]\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
        "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
        "        return out_string\n",
        "\n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
        "        by concatenating and adding special tokens.\n",
        "        A KoBERT sequence has the following format:\n",
        "            single sequence: [CLS] X [SEP]\n",
        "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
        "        \"\"\"\n",
        "        if token_ids_1 is None:\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        sep = [self.sep_token_id]\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        "\n",
        "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
        "        \"\"\"\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
        "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
        "        Args:\n",
        "            token_ids_0: list of ids (must not contain special tokens)\n",
        "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
        "                for sequence pairs\n",
        "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
        "                special tokens for the model\n",
        "        Returns:\n",
        "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
        "        \"\"\"\n",
        "\n",
        "        if already_has_special_tokens:\n",
        "            if token_ids_1 is not None:\n",
        "                raise ValueError(\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\n",
        "                    \"ids is already formated with special tokens for the model.\"\n",
        "                )\n",
        "            return list(\n",
        "                map(\n",
        "                    lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0,\n",
        "                    token_ids_0,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if token_ids_1 is not None:\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
        "\n",
        "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
        "        A KoBERT sequence pair mask has the following format:\n",
        "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
        "        | first sequence    | second sequence\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return len(cls + token_ids_0 + sep) * [0]\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
        "\n",
        "    def save_vocabulary(self, save_directory):\n",
        "        \"\"\"Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
        "        to a directory.\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(save_directory):\n",
        "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
        "            return\n",
        "\n",
        "        # 1. Save sentencepiece model\n",
        "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
        "\n",
        "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
        "            copyfile(self.vocab_file, out_vocab_model)\n",
        "\n",
        "        # 2. Save vocab.txt\n",
        "        index = 0\n",
        "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
        "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
        "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\n",
        "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
        "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
        "                    )\n",
        "                    index = token_index\n",
        "                writer.write(token + \"\\n\")\n",
        "                index += 1\n",
        "\n",
        "        return out_vocab_model, out_vocab_txt"
      ],
      "metadata": {
        "id": "lOZ4_GDDacSQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self, max_len: int):\n",
        "        self.tokenizer = KoBertTokenizer.from_pretrained(\"monologg/kobert\")\n",
        "        self.max_len = max_len\n",
        "        self.pad_token_id = 0\n",
        "\n",
        "    def get_input_features(\n",
        "        self, sentence: List[str], tags: List[str]\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"문장과 띄어쓰기 tagging에 대해 feature로 변환한다.\n",
        "\n",
        "        Args:\n",
        "            sentence: 문장\n",
        "            tags: 띄어쓰기 tagging\n",
        "\n",
        "        Returns:\n",
        "            feature를 리턴한다.\n",
        "            input_ids, attention_mask, token_type_ids, slot_labels\n",
        "        \"\"\"\n",
        "\n",
        "        input_tokens = []\n",
        "        slot_label_ids = []\n",
        "\t\t\t\t\t\n",
        "        # tokenize\n",
        "        for word, tag in zip(sentence, tags):\n",
        "            tokens = self.tokenizer.tokenize(word)\n",
        "\n",
        "            if len(tokens) == 0:\n",
        "                tokens = self.tokenizer.unk_token\n",
        "\n",
        "            input_tokens.extend(tokens)\n",
        "\n",
        "            for i in range(len(tokens)):\n",
        "                if i == 0:\n",
        "                    slot_label_ids.extend([tag])\n",
        "                else:\n",
        "                    slot_label_ids.extend([self.pad_token_id])\n",
        "\n",
        "        # max_len보다 길이가 길면 뒤에 자르기\n",
        "        if len(input_tokens) > self.max_len - 2:\n",
        "            input_tokens = input_tokens[: self.max_len - 2]\n",
        "            slot_label_ids = slot_label_ids[: self.max_len - 2]\n",
        "\n",
        "        # cls, sep 추가\n",
        "        input_tokens = (\n",
        "            [self.tokenizer.cls_token] + input_tokens + [self.tokenizer.sep_token]\n",
        "        )\n",
        "        slot_label_ids = [self.pad_token_id] + slot_label_ids + [self.pad_token_id]\n",
        "\n",
        "        # token을 id로 변환\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        token_type_ids = [0] * len(input_ids)\n",
        "\n",
        "        # padding\n",
        "        pad_len = self.max_len - len(input_tokens)\n",
        "        input_ids = input_ids + ([self.tokenizer.pad_token_id] * pad_len)\n",
        "        slot_label_ids = slot_label_ids + ([self.pad_token_id] * pad_len)\n",
        "        attention_mask = attention_mask + ([0] * pad_len)\n",
        "        token_type_ids = token_type_ids + ([0] * pad_len)\n",
        "\n",
        "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "        attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
        "        token_type_ids = torch.tensor(token_type_ids, dtype=torch.long)\n",
        "        slot_label_ids = torch.tensor(slot_label_ids, dtype=torch.long)\n",
        "\n",
        "        return input_ids, attention_mask, token_type_ids, slot_label_ids"
      ],
      "metadata": {
        "id": "JYLjOPIlTLit"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = Preprocessor(config.max_len)\n",
        "dataset = CorpusDataset(data_path, preprocessor.get_input_features)"
      ],
      "metadata": {
        "id": "pYVtHNZXZWyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 생성"
      ],
      "metadata": {
        "id": "uE41yxbCazAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "0Tl9DPgfa-iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install seqeval"
      ],
      "metadata": {
        "id": "CiSH4mg2bYyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install utils"
      ],
      "metadata": {
        "id": "rEOggOOYbn8v",
        "outputId": "7698481d-6d08-4d50-987e-c31cafb77496",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting utils\n",
            "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from transformers import BertConfig, BertModel, AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from seqeval.metrics import f1_score\n",
        "\n",
        "\n",
        "class SpacingBertModel(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        config,\n",
        "        dataset: CorpusDataset,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.dataset = dataset\n",
        "        self.slot_labels_type = [\"UNK\", \"PAD\", \"B\", \"I\"]\n",
        "        self.pad_token_id = 0\n",
        "\n",
        "        self.bert_config = BertConfig.from_pretrained(\n",
        "            self.config.bert_model, num_labels=len(self.slot_labels_type)\n",
        "        )\n",
        "        self.model = BertModel.from_pretrained(\n",
        "            self.config.bert_model, config=self.bert_config\n",
        "        )\n",
        "        self.dropout = nn.Dropout(self.config.dropout_rate)\n",
        "        self.linear = nn.Linear(\n",
        "            self.bert_config.hidden_size, len(self.slot_labels_type)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )\n",
        "\n",
        "        x = outputs[0]\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "\n",
        "        input_ids, attention_mask, token_type_ids, slot_label_ids = batch\n",
        "\n",
        "        outputs = self(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )\n",
        "\n",
        "        loss = self._calculate_loss(outputs, slot_labels)\n",
        "        tensorboard_logs = {\"train_loss\": loss}\n",
        "\n",
        "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "\n",
        "        input_ids, attention_mask, token_type_ids, slot_label_ids = batch\n",
        "\n",
        "        outputs = self(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )\n",
        "\n",
        "        loss = self._calculate_loss(outputs, slot_label_ids)\n",
        "        pred_slot_labels, gt_slot_labels = self._convert_ids_to_labels(\n",
        "            outputs, slot_label_ids\n",
        "        )\n",
        "\n",
        "        val_f1 = self._f1_score(gt_slot_labels, pred_slot_labels)\n",
        "\n",
        "        return {\"val_loss\": loss, \"val_f1\": val_f1}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        val_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "        val_f1 = torch.stack([x[\"val_f1\"] for x in outputs]).mean()\n",
        "\n",
        "        tensorboard_log = {\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_f1\": val_f1,\n",
        "        }\n",
        "\n",
        "        return {\"val_loss\": val_loss, \"progress_bar\": tensorboard_log}\n",
        "\n",
        "    def test_step(self, batch, batch_nb):\n",
        "\n",
        "        input_ids, attention_mask, token_type_ids, slot_label_ids = batch\n",
        "\n",
        "        outputs = self(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )\n",
        "\n",
        "        pred_slot_labels, gt_slot_labels = self._convert_ids_to_labels(\n",
        "            outputs, slot_label_ids\n",
        "        )\n",
        "\n",
        "        test_f1 = self._f1_score(gt_slot_labels, pred_slot_labels)\n",
        "\n",
        "        test_step_outputs = {\n",
        "            \"test_f1\": test_f1,\n",
        "        }\n",
        "\n",
        "        return test_step_outputs\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        test_f1 = torch.stack([x[\"test_f1\"] for x in outputs]).mean()\n",
        "\n",
        "        test_step_outputs = {\n",
        "            \"test_f1\": test_f1,\n",
        "        }\n",
        "\n",
        "        return test_step_outputs\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return AdamW(self.model.parameters(), lr=2e-5, eps=1e-8)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.dataset[\"train\"], batch_size=self.config.train_batch_size)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.dataset[\"val\"], batch_size=self.config.eval_batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.dataset[\"test\"], batch_size=self.config.eval_batch_size)\n",
        "\n",
        "    def _calculate_loss(self, outputs, labels):\n",
        "        active_logits = outputs.view(-1, len(self.slot_labels_type))\n",
        "        active_labels = labels.view(-1)\n",
        "        loss = F.cross_entropy(active_logits, active_labels)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _f1_score(self, gt_slot_labels, pred_slot_labels):\n",
        "        return torch.tensor(\n",
        "            f1_score(gt_slot_labels, pred_slot_labels), dtype=torch.float32\n",
        "        )\n",
        "\n",
        "    def _convert_ids_to_labels(self, outputs, slot_labels):\n",
        "        _, y_hat = torch.max(outputs, dim=2)\n",
        "        y_hat = y_hat.detach().cpu().numpy()\n",
        "        slot_label_ids = slot_labels.detach().cpu().numpy()\n",
        "\n",
        "        slot_label_map = {i: label for i, label in enumerate(self.slot_labels_type)}\n",
        "        slot_gt_labels = [[] for _ in range(slot_label_ids.shape[0])]\n",
        "        slot_pred_labels = [[] for _ in range(slot_label_ids.shape[0])]\n",
        "\n",
        "        for i in range(slot_label_ids.shape[0]):\n",
        "            for j in range(slot_label_ids.shape[1]):\n",
        "                if slot_label_ids[i, j] != self.pad_token_id:\n",
        "                    slot_gt_labels[i].append(slot_label_map[slot_label_ids[i][j]])\n",
        "                    slot_pred_labels[i].append(slot_label_map[y_hat[i][j]])\n",
        "\n",
        "        return slot_pred_labels, slot_gt_labels"
      ],
      "metadata": {
        "id": "-N8IkQtea0av"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 학습"
      ],
      "metadata": {
        "id": "yJEtlPbjpXL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_path = '/content/drive/MyDrive/train_test/train.txt'\n",
        "test_data_path = '/content/drive/MyDrive/train_test/test.txt'"
      ],
      "metadata": {
        "id": "XRqdvLG4pefs"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install omegaconf "
      ],
      "metadata": {
        "id": "UewCAW5eqR2E",
        "outputId": "fb70d0a9-ef3e-4ca1-98aa-a636767137ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▍                           | 10 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 20 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 30 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 40 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 51 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 61 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 71 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 74 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from omegaconf) (6.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[?25l\r\u001b[K     |███                             | 10 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 20 kB 32.6 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 30 kB 41.7 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 40 kB 47.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 51 kB 52.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 61 kB 58.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 71 kB 54.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 81 kB 55.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 92 kB 59.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 102 kB 19.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112 kB 19.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=3bb4a82a12a04379d611eafb7785b9d50428b4ca255c32b1221d3f4e5d7a7938\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf\n",
            "Successfully installed antlr4-python3-runtime-4.8 omegaconf-2.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from omegaconf import OmegaConf\n",
        "\n",
        "config = OmegaConf.load('/content/drive/MyDrive/train_config.yaml')"
      ],
      "metadata": {
        "id": "xes-65_6qLMf"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = {}\n",
        "dataset[\"train\"] = CorpusDataset(\n",
        "    train_data_path, Preprocessor.get_input_features\n",
        ")\n",
        "dataset[\"test\"] = CorpusDataset(\n",
        "    test_data_path, Preprocessor.get_input_features\n",
        ")\n",
        "\n",
        "\n",
        "bert_finetuner = SpacingBertModel(config, dataset)"
      ],
      "metadata": {
        "id": "vwE5rSYGpYfd"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "logger = TensorBoardLogger(\n",
        "    save_dir=os.path.join(config.log_path, config.task), version=1, name=config.task\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=\"checkpoints/\"+ config.task + \"/{epoch}_{val_loss:35f}\",\n",
        "    verbose=True,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    save_top_k=3,\n",
        "    prefix=\"\",\n",
        ")\n",
        "\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0.0001,\n",
        "    patience=3,\n",
        "    verbose=False,\n",
        "    mode=\"min\",\n",
        ")"
      ],
      "metadata": {
        "id": "UaMdwERtqb0Q"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = pl.Trainer(\n",
        "    gpus=config.gpus,\n",
        "    distributed_backend=config.distributed_backend,\n",
        "    checkpoint_callback=checkpoint_callback,\n",
        "    early_stop_callback=early_stop_callback,\n",
        "    logger=logger,\n",
        ")\n",
        "\n",
        "trainer.fit(bert_finetuner)\n",
        "trainer.test()"
      ],
      "metadata": {
        "id": "OAw-88c_q_Dn",
        "outputId": "100553af-0922-4341-ff3c-fb7502497d31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-ef193e9a6aa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mearly_stop_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stop_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\u001b[0m in \u001b[0;36minsert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# all args were already moved to kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minsert_env_defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'distributed_backend'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zdJTcVYxsegv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sHhCIplMsm_9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "CP2_KoSpacing.ipynb",
      "provenance": [],
      "mount_file_id": "1GOzxSVo7c6s_VIL1IPiy2rgupLE9ZRqc",
      "authorship_tag": "ABX9TyOeNwNqWFJ5QcClGaVDWai+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}