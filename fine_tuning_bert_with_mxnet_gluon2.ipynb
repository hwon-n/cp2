{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwon-n/cp2/blob/hyewon/fine_tuning_bert_with_mxnet_gluon2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff6E-Vjaj-dd"
      },
      "source": [
        "# BERT fine funing\n",
        "\n",
        "[참고](https://www.kaggle.com/code/jt0321/fine-tuning-bert-with-mxnet-gluon/notebook)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P86vJeHkfWv",
        "outputId": "d3456d3c-297d-45eb-c474-cda2905c6f20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ms2DqJHbyD2v",
        "outputId": "539ae7a7-4140-4a24-db13-f9fb3a2c3356"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`!nvcc --version` 으로 cuda 버전 확인 후 [mxnet](https://pypi.org/project/mxnet/) 들어가서 맞는 mxnet-cu1xx 설치하기"
      ],
      "metadata": {
        "id": "Bxu33NkMz3Rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mxnet-cu110"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQTzWMwyyL-t",
        "outputId": "0b41e9fc-7151-4840-e130-b460ca93f824"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mxnet-cu110\n",
            "  Downloading mxnet_cu110-1.9.0-py3-none-manylinux2014_x86_64.whl (325.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 325.4 MB 29 kB/s \n",
            "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu110) (1.21.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu110) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu110) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu110) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu110) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu110) (3.0.4)\n",
            "Installing collected packages: graphviz, mxnet-cu110\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-cu110-1.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2rIyOuX2W3tn",
        "outputId": "1008949c-6c56-4ba3-fc12-06963091f63f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-br65gna3\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-br65gna3\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.21.27-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gluonnlp>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (0.10.0)\n",
            "Collecting mxnet>=1.4.0\n",
            "  Downloading mxnet-1.9.0-py3-none-manylinux2014_x86_64.whl (47.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.3 MB 1.8 MB/s \n",
            "\u001b[?25hCollecting onnxruntime==1.8.0\n",
            "  Downloading onnxruntime-1.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 36.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (0.1.96)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (1.10.0+cu111)\n",
            "Collecting transformers>=4.8.1\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 57.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (2.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (1.21.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (3.17.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (0.29.28)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (21.3)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->kobert==0.2.3) (0.8.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->kobert==0.2.3) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (2021.10.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->kobert==0.2.3) (3.10.0.2)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 50.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (4.63.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (3.6.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 61.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 63.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp>=0.6.0->kobert==0.2.3) (3.0.7)\n",
            "Collecting botocore<1.25.0,>=1.24.27\n",
            "  Downloading botocore-1.24.27-py3-none-any.whl (8.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 36.4 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.4 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 76.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.27->boto3->kobert==0.2.3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.27->boto3->kobert==0.2.3) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.8.1->kobert==0.2.3) (3.7.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (1.1.0)\n",
            "Building wheels for collected packages: kobert\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15674 sha256=9c87c0168600343ce1ca77183708a62e59819732e98c56102006cba8c0125973\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7neh3e0l/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0\n",
            "Successfully built kobert\n",
            "Installing collected packages: urllib3, jmespath, pyyaml, botocore, tokenizers, sacremoses, s3transfer, huggingface-hub, transformers, onnxruntime, mxnet, boto3, kobert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.21.27 botocore-1.24.27 huggingface-hub-0.4.0 jmespath-1.0.0 kobert-0.2.3 mxnet-1.9.0 onnxruntime-1.8.0 pyyaml-6.0 s3transfer-0.5.2 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.7/dist-packages/boto3-1.21.27.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n",
            "/usr/local/lib/python3.7/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.7/dist-packages/botocore-1.24.27.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n",
            "/usr/local/lib/python3.7/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.7/dist-packages/huggingface_hub-0.4.0.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n",
            "/usr/local/lib/python3.7/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.7/dist-packages/jmespath-1.0.0.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n",
            "/usr/local/lib/python3.7/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.7/dist-packages/kobert-0.2.3.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n",
            "/usr/local/lib/python3.7/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.7/dist-packages/mxnet-1.9.0.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n",
            "/usr/local/lib/python3.7/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.7/dist-packages/onnxruntime-1.8.0.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n",
            "/usr/local/lib/python3.7/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.7/dist-packages/PyYAML-6.0.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n",
            "/usr/local/lib/python3.7/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.7/dist-packages/s3transfer-0.5.2.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n",
            "/usr/local/lib/python3.7/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.7/dist-packages/sacremoses-0.0.49.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n",
            "/usr/local/lib/python3.7/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.7/dist-packages/tokenizers-0.11.6.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n",
            "/usr/local/lib/python3.7/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.7/dist-packages/transformers-4.17.0.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n",
            "/usr/local/lib/python3.7/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.7/dist-packages/urllib3-1.25.11.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "mxnet",
                  "urllib3",
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13BO96DlVIrJ",
        "outputId": "9d24ce44-18c9-4e6c-f8dc-8bc316f74942"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 7.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.7/dist-packages/sentencepiece-0.1.96.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gluonnlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uei3tmi-kFSH",
        "outputId": "3d7b94c6-479c-4ab2-99d3-c34c62919cac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gluonnlp\n",
            "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 40 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 51 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 61 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 71 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 81 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 92 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 102 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 112 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 122 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 133 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 143 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 153 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 163 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 174 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 184 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 194 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 204 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 215 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 225 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 235 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 245 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 256 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 266 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 276 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 286 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 296 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 307 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 317 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 327 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 337 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 344 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.21.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.28)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (3.0.7)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595733 sha256=9d113bfef5abfc46227494fe835819229ff65acb6d42801d8b8e04fa202c6575\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2Oesm8kj-dk",
        "outputId": "b57c5180-b9ca-4fbb-ddf2-de3bb1eb5722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/fft/__init__.py:97: DeprecationWarning: The module numpy.dual is deprecated.  Instead of using dual, use the functions directly from numpy or scipy.\n",
            "  from numpy.dual import register_func\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/local/lib/python3.7/dist-packages/mxnet/numpy/utils.py:37: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  bool = onp.bool\n",
            "/usr/local/lib/python3.7/dist-packages/mxnet/numpy/fallback.py:143: DeprecationWarning: In accordance with NEP 32, the function mirr was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
            "  mirr = onp.mirr\n",
            "/usr/local/lib/python3.7/dist-packages/mxnet/numpy/fallback.py:160: DeprecationWarning: In accordance with NEP 32, the function npv was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
            "  npv = onp.npv\n",
            "/usr/local/lib/python3.7/dist-packages/mxnet/numpy/fallback.py:164: DeprecationWarning: In accordance with NEP 32, the function pmt was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
            "  pmt = onp.pmt\n",
            "/usr/local/lib/python3.7/dist-packages/mxnet/numpy/fallback.py:173: DeprecationWarning: In accordance with NEP 32, the function ppmt was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
            "  ppmt = onp.ppmt\n",
            "/usr/local/lib/python3.7/dist-packages/mxnet/numpy/fallback.py:176: DeprecationWarning: In accordance with NEP 32, the function pv was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
            "  pv = onp.pv\n",
            "/usr/local/lib/python3.7/dist-packages/mxnet/numpy/fallback.py:177: DeprecationWarning: In accordance with NEP 32, the function rate was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: https://pypi.org/project/numpy-financial\n",
            "  rate = onp.rate\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n",
            "/usr/local/lib/python3.7/dist-packages/numba/core/types/__init__.py:108: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  long_ = _make_signed(np.long)\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='once')\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import multiprocessing\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm_notebook, tqdm\n",
        "\n",
        "import mxnet as mx\n",
        "from mxnet import gluon\n",
        "from mxnet.gluon import Block\n",
        "from mxnet.gluon import nn\n",
        "import gluonnlp as nlp\n",
        "from gluonnlp.data import BERTTokenizer, BERTSentenceTransform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VGDnJxHZj-dp"
      },
      "outputs": [],
      "source": [
        "train_tsv = '/content/drive/MyDrive/news_class9x13000/train.tsv'\n",
        "test_tsv = '/content/drive/MyDrive/news_class9x13000/test.tsv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "BHccPxtPj-dq",
        "outputId": "19c6c646-8ebc-40d5-b3cc-3635233f8b77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   target                                             prompt  \\\n",
              "0       1                  프로크루스테스의 침대라는 그리스신화에 나오는 이야기가 있다.   \n",
              "1       0  이 이야기는 프로크루스테스라는 사람이 지나가는 나그네를 자신의 집으로 잡아와서 침대...   \n",
              "2       0                               결국 침대에 사람을 맞춘다는 얘기다.   \n",
              "3       0       올해부터 실시하고 있는 활동보조서비스 개악 지침도 이것과 다르지 않게 잔인하다.   \n",
              "4       1  장애인활동보조서비스는 장애로 인해 일상생활의 어려움을 겪는 사람들의 원활한 일상생활...   \n",
              "\n",
              "                                                next  \n",
              "0  이 이야기는 프로크루스테스라는 사람이 지나가는 나그네를 자신의 집으로 잡아와서 침대...  \n",
              "1  또, 안전행정부와 기획재정부는 전 부처와 공공기관을 대상으로 여름휴가 하루 더 가기...  \n",
              "2       올해부터 실시하고 있는 활동보조서비스 개악 지침도 이것과 다르지 않게 잔인하다.  \n",
              "3  흥선대원군은 이홍장의 심문을 받은 후 바오딩(保定)으로 옮겨져 4년간 중국에서 불우...  \n",
              "4  2007년부터 시작한 이 서비스 제도는 전국의 중증장애인들이 국가를 상대로 치열한 ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8d89b8a8-63b0-4d96-9aee-30449636111d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>prompt</th>\n",
              "      <th>next</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>프로크루스테스의 침대라는 그리스신화에 나오는 이야기가 있다.</td>\n",
              "      <td>이 이야기는 프로크루스테스라는 사람이 지나가는 나그네를 자신의 집으로 잡아와서 침대...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>이 이야기는 프로크루스테스라는 사람이 지나가는 나그네를 자신의 집으로 잡아와서 침대...</td>\n",
              "      <td>또, 안전행정부와 기획재정부는 전 부처와 공공기관을 대상으로 여름휴가 하루 더 가기...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>결국 침대에 사람을 맞춘다는 얘기다.</td>\n",
              "      <td>올해부터 실시하고 있는 활동보조서비스 개악 지침도 이것과 다르지 않게 잔인하다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>올해부터 실시하고 있는 활동보조서비스 개악 지침도 이것과 다르지 않게 잔인하다.</td>\n",
              "      <td>흥선대원군은 이홍장의 심문을 받은 후 바오딩(保定)으로 옮겨져 4년간 중국에서 불우...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>장애인활동보조서비스는 장애로 인해 일상생활의 어려움을 겪는 사람들의 원활한 일상생활...</td>\n",
              "      <td>2007년부터 시작한 이 서비스 제도는 전국의 중증장애인들이 국가를 상대로 치열한 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d89b8a8-63b0-4d96-9aee-30449636111d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8d89b8a8-63b0-4d96-9aee-30449636111d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8d89b8a8-63b0-4d96-9aee-30449636111d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train_df = pd.read_csv(train_tsv, delimiter = '\\t', header = None)\n",
        "train_df.columns = ['target', 'prompt', 'next']\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "tNzgHKihj-dr",
        "outputId": "febaa205-ff7d-4a1a-e947-13875aa4febd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   target                       prompt  \\\n",
              "0       1              보름달 뜨면 구름 자주 끼고   \n",
              "1       0           꽃이 활짝 피면 바람이 불어대지.   \n",
              "2       1              세상일이란 모두 이런 거야.   \n",
              "3       0         나 홀로 웃는 까닭 아는 이 없을걸.   \n",
              "4       1  정약용이 지은 독소(獨笑·홀로 웃다)라는 시이다.   \n",
              "\n",
              "                                        next  \n",
              "0                         꽃이 활짝 피면 바람이 불어대지.  \n",
              "1                   ◈ 안쓰고 쌓아두면 세금...기업소득환류세제  \n",
              "2                       나 홀로 웃는 까닭 아는 이 없을걸.  \n",
              "3                  권력기관에서 경력을 쌓은 인물들도 적지 않다.  \n",
              "4  이렇게 인간사 누구나 한 가지씩 걱정거리를 안고 사는 게 우리네 인생이다.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5a3f115c-0272-4629-abad-33977c86c3eb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>prompt</th>\n",
              "      <th>next</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>보름달 뜨면 구름 자주 끼고</td>\n",
              "      <td>꽃이 활짝 피면 바람이 불어대지.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>꽃이 활짝 피면 바람이 불어대지.</td>\n",
              "      <td>◈ 안쓰고 쌓아두면 세금...기업소득환류세제</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>세상일이란 모두 이런 거야.</td>\n",
              "      <td>나 홀로 웃는 까닭 아는 이 없을걸.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>나 홀로 웃는 까닭 아는 이 없을걸.</td>\n",
              "      <td>권력기관에서 경력을 쌓은 인물들도 적지 않다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>정약용이 지은 독소(獨笑·홀로 웃다)라는 시이다.</td>\n",
              "      <td>이렇게 인간사 누구나 한 가지씩 걱정거리를 안고 사는 게 우리네 인생이다.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a3f115c-0272-4629-abad-33977c86c3eb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5a3f115c-0272-4629-abad-33977c86c3eb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5a3f115c-0272-4629-abad-33977c86c3eb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "test_df = pd.read_csv(test_tsv, delimiter = '\\t', header = None)\n",
        "test_df.columns = ['target', 'prompt', 'next']\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M1BF3u_j-dt"
      },
      "source": [
        "### classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "o-UjGJxBj-du"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(Block):\n",
        "    \"\"\"Model for sentence (pair) classification task with BERT.\n",
        "\n",
        "    The model feeds token ids and token type ids into BERT to get the\n",
        "    pooled BERT sequence representation, then apply a Dense layer for\n",
        "    classification.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bert: BERTModel\n",
        "        Bidirectional encoder with transformer.\n",
        "    num_classes : int, default is 2\n",
        "        The number of target classes.\n",
        "    dropout : float or None, default 0.0.\n",
        "        Dropout probability for the bert output.\n",
        "    prefix : str or None\n",
        "        See document of `mx.gluon.Block`.\n",
        "    params : ParameterDict or None\n",
        "        See document of `mx.gluon.Block`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 num_classes=2,\n",
        "                 dropout=0.0,\n",
        "                 prefix=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__(prefix=prefix, params=params)\n",
        "        self.bert = bert\n",
        "        with self.name_scope():\n",
        "            self.classifier = nn.HybridSequential(prefix=prefix)\n",
        "            if dropout:\n",
        "                self.classifier.add(nn.Dropout(rate=dropout))\n",
        "            self.classifier.add(nn.Dense(units=num_classes))\n",
        "\n",
        "    def forward(self, inputs, token_types, valid_length=None):  # pylint: disable=arguments-differ\n",
        "        \"\"\"Generate the unnormalized score for the given the input sequences.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inputs : NDArray, shape (batch_size, seq_length)\n",
        "            Input words for the sequences.\n",
        "        token_types : NDArray, shape (batch_size, seq_length)\n",
        "            Token types for the sequences, used to indicate whether the word belongs to the\n",
        "            first sentence or the second one.\n",
        "        valid_length : NDArray or None, shape (batch_size)\n",
        "            Valid length of the sequence. This is used to mask the padded tokens.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        outputs : NDArray\n",
        "            Shape (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        _, pooler_out = self.bert(inputs, token_types, valid_length)\n",
        "        return self.classifier(pooler_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7jI96t59j-dv"
      },
      "outputs": [],
      "source": [
        "class BERTDatasetTransform(object):\n",
        "    \"\"\"Dataset Transformation for BERT-style Sentence Classification or Regression.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tokenizer : BERTTokenizer.\n",
        "        Tokenizer for the sentences.\n",
        "    max_seq_length : int.\n",
        "        Maximum sequence length of the sentences.\n",
        "    labels : list of int , float or None. defaults None\n",
        "        List of all label ids for the classification task and regressing task.\n",
        "        If labels is None, the default task is regression\n",
        "    pad : bool, default True\n",
        "        Whether to pad the sentences to maximum length.\n",
        "    pair : bool, default True\n",
        "        Whether to transform sentences or sentence pairs.\n",
        "    label_dtype: int32 or float32, default float32\n",
        "        label_dtype = int32 for classification task\n",
        "        label_dtype = float32 for regression task\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 tokenizer,\n",
        "                 max_seq_length,\n",
        "                 class_labels=None,\n",
        "                 pad=True,\n",
        "                 pair=True,\n",
        "                 has_label=True):\n",
        "        self.class_labels = class_labels\n",
        "        self.has_label = has_label\n",
        "        self._label_dtype = 'int32' if class_labels else 'float32'\n",
        "        if has_label and class_labels:\n",
        "            self._label_map = {}\n",
        "            for (i, label) in enumerate(class_labels):\n",
        "                self._label_map[label] = i\n",
        "        self._bert_xform = BERTSentenceTransform(\n",
        "            tokenizer, max_seq_length, pad=pad, pair=pair)\n",
        "\n",
        "    def __call__(self, line):\n",
        "        \"\"\"Perform transformation for sequence pairs or single sequences.\n",
        "\n",
        "        The transformation is processed in the following steps:\n",
        "        - tokenize the input sequences\n",
        "        - insert [CLS], [SEP] as necessary\n",
        "        - generate type ids to indicate whether a token belongs to the first\n",
        "          sequence or the second sequence.\n",
        "        - generate valid length\n",
        "\n",
        "        For sequence pairs, the input is a tuple of 3 strings:\n",
        "        text_a, text_b and label.\n",
        "\n",
        "        Inputs:\n",
        "            text_a: 'is this jacksonville ?'\n",
        "            text_b: 'no it is not'\n",
        "            label: '0'\n",
        "        Tokenization:\n",
        "            text_a: 'is this jack ##son ##ville ?'\n",
        "            text_b: 'no it is not .'\n",
        "        Processed:\n",
        "            tokens:  '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'\n",
        "            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "            valid_length: 14\n",
        "            label: 0\n",
        "\n",
        "        For single sequences, the input is a tuple of 2 strings: text_a and label.\n",
        "        Inputs:\n",
        "            text_a: 'the dog is hairy .'\n",
        "            label: '1'\n",
        "        Tokenization:\n",
        "            text_a: 'the dog is hairy .'\n",
        "        Processed:\n",
        "            text_a:  '[CLS] the dog is hairy . [SEP]'\n",
        "            type_ids: 0     0   0   0  0     0 0\n",
        "            valid_length: 7\n",
        "            label: 1\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        line: tuple of str\n",
        "            Input strings. For sequence pairs, the input is a tuple of 3 strings:\n",
        "            (text_a, text_b, label). For single sequences, the input is a tuple\n",
        "            of 2 strings: (text_a, label).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.array: input token ids in 'int32', shape (batch_size, seq_length)\n",
        "        np.array: valid length in 'int32', shape (batch_size,)\n",
        "        np.array: input token type ids in 'int32', shape (batch_size, seq_length)\n",
        "        np.array: classification task: label id in 'int32', shape (batch_size, 1),\n",
        "            regression task: label in 'float32', shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        if self.has_label:\n",
        "            input_ids, valid_length, segment_ids = self._bert_xform(line[:-1])\n",
        "            label = line[-1]\n",
        "            # map to int if class labels are available\n",
        "            if self.class_labels:\n",
        "                label = self._label_map[label]\n",
        "            label = np.array([label], dtype=self._label_dtype)\n",
        "            return input_ids, valid_length, segment_ids, label\n",
        "        else:\n",
        "            return self._bert_xform(line)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PqxVS8Rj-dx"
      },
      "source": [
        "### preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bZQIJY2vj-dx"
      },
      "outputs": [],
      "source": [
        "ctx = mx.gpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "njLRPucuj-dy"
      },
      "outputs": [],
      "source": [
        "max_len = 128\n",
        "pad = True\n",
        "pair = True\n",
        "\n",
        "epochs = 3\n",
        "batch_size = 32\n",
        "#optimizer='bertadam'\n",
        "class_labels=[0, 1]\n",
        "lr = 1e-6\n",
        "epsilon = 1e-06\n",
        "warmup_ratio = 0.1\n",
        "log_interval = 1000\n",
        "accumulate = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kouIH-zj-dy",
        "outputId": "9c62681e-6a2c-458e-fdbe-2fbc36b6bc59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERTModel(\n",
            "  (encoder): BERTEncoder(\n",
            "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "    (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "    (transformer_cells): HybridSequential(\n",
            "      (0): BERTEncoderCell(\n",
            "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        (attention_cell): DotProductSelfAttentionCell(\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        )\n",
            "        (proj): Dense(768 -> 768, linear)\n",
            "        (ffn): PositionwiseFFN(\n",
            "          (ffn_1): Dense(768 -> 3072, linear)\n",
            "          (activation): GELU()\n",
            "          (ffn_2): Dense(3072 -> 768, linear)\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "        )\n",
            "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "      )\n",
            "      (1): BERTEncoderCell(\n",
            "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        (attention_cell): DotProductSelfAttentionCell(\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        )\n",
            "        (proj): Dense(768 -> 768, linear)\n",
            "        (ffn): PositionwiseFFN(\n",
            "          (ffn_1): Dense(768 -> 3072, linear)\n",
            "          (activation): GELU()\n",
            "          (ffn_2): Dense(3072 -> 768, linear)\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "        )\n",
            "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "      )\n",
            "      (2): BERTEncoderCell(\n",
            "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        (attention_cell): DotProductSelfAttentionCell(\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        )\n",
            "        (proj): Dense(768 -> 768, linear)\n",
            "        (ffn): PositionwiseFFN(\n",
            "          (ffn_1): Dense(768 -> 3072, linear)\n",
            "          (activation): GELU()\n",
            "          (ffn_2): Dense(3072 -> 768, linear)\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "        )\n",
            "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "      )\n",
            "      (3): BERTEncoderCell(\n",
            "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        (attention_cell): DotProductSelfAttentionCell(\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        )\n",
            "        (proj): Dense(768 -> 768, linear)\n",
            "        (ffn): PositionwiseFFN(\n",
            "          (ffn_1): Dense(768 -> 3072, linear)\n",
            "          (activation): GELU()\n",
            "          (ffn_2): Dense(3072 -> 768, linear)\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "        )\n",
            "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "      )\n",
            "      (4): BERTEncoderCell(\n",
            "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        (attention_cell): DotProductSelfAttentionCell(\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        )\n",
            "        (proj): Dense(768 -> 768, linear)\n",
            "        (ffn): PositionwiseFFN(\n",
            "          (ffn_1): Dense(768 -> 3072, linear)\n",
            "          (activation): GELU()\n",
            "          (ffn_2): Dense(3072 -> 768, linear)\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "        )\n",
            "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "      )\n",
            "      (5): BERTEncoderCell(\n",
            "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        (attention_cell): DotProductSelfAttentionCell(\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        )\n",
            "        (proj): Dense(768 -> 768, linear)\n",
            "        (ffn): PositionwiseFFN(\n",
            "          (ffn_1): Dense(768 -> 3072, linear)\n",
            "          (activation): GELU()\n",
            "          (ffn_2): Dense(3072 -> 768, linear)\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "        )\n",
            "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "      )\n",
            "      (6): BERTEncoderCell(\n",
            "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        (attention_cell): DotProductSelfAttentionCell(\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        )\n",
            "        (proj): Dense(768 -> 768, linear)\n",
            "        (ffn): PositionwiseFFN(\n",
            "          (ffn_1): Dense(768 -> 3072, linear)\n",
            "          (activation): GELU()\n",
            "          (ffn_2): Dense(3072 -> 768, linear)\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "        )\n",
            "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "      )\n",
            "      (7): BERTEncoderCell(\n",
            "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        (attention_cell): DotProductSelfAttentionCell(\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        )\n",
            "        (proj): Dense(768 -> 768, linear)\n",
            "        (ffn): PositionwiseFFN(\n",
            "          (ffn_1): Dense(768 -> 3072, linear)\n",
            "          (activation): GELU()\n",
            "          (ffn_2): Dense(3072 -> 768, linear)\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "        )\n",
            "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "      )\n",
            "      (8): BERTEncoderCell(\n",
            "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        (attention_cell): DotProductSelfAttentionCell(\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        )\n",
            "        (proj): Dense(768 -> 768, linear)\n",
            "        (ffn): PositionwiseFFN(\n",
            "          (ffn_1): Dense(768 -> 3072, linear)\n",
            "          (activation): GELU()\n",
            "          (ffn_2): Dense(3072 -> 768, linear)\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "        )\n",
            "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "      )\n",
            "      (9): BERTEncoderCell(\n",
            "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        (attention_cell): DotProductSelfAttentionCell(\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        )\n",
            "        (proj): Dense(768 -> 768, linear)\n",
            "        (ffn): PositionwiseFFN(\n",
            "          (ffn_1): Dense(768 -> 3072, linear)\n",
            "          (activation): GELU()\n",
            "          (ffn_2): Dense(3072 -> 768, linear)\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "        )\n",
            "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "      )\n",
            "      (10): BERTEncoderCell(\n",
            "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        (attention_cell): DotProductSelfAttentionCell(\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        )\n",
            "        (proj): Dense(768 -> 768, linear)\n",
            "        (ffn): PositionwiseFFN(\n",
            "          (ffn_1): Dense(768 -> 3072, linear)\n",
            "          (activation): GELU()\n",
            "          (ffn_2): Dense(3072 -> 768, linear)\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "        )\n",
            "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "      )\n",
            "      (11): BERTEncoderCell(\n",
            "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        (attention_cell): DotProductSelfAttentionCell(\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "        )\n",
            "        (proj): Dense(768 -> 768, linear)\n",
            "        (ffn): PositionwiseFFN(\n",
            "          (ffn_1): Dense(768 -> 3072, linear)\n",
            "          (activation): GELU()\n",
            "          (ffn_2): Dense(3072 -> 768, linear)\n",
            "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
            "          (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "        )\n",
            "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (word_embed): HybridSequential(\n",
            "    (0): Embedding(8002 -> 768, float32)\n",
            "  )\n",
            "  (token_type_embed): HybridSequential(\n",
            "    (0): Embedding(2 -> 768, float32)\n",
            "  )\n",
            "  (pooler): Dense(768 -> 768, Activation(tanh))\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "bert_base, vocabulary = nlp.model.get_model('bert_12_768_12', \n",
        "                                             dataset_name='kobert_news_wiki_ko_cased',\n",
        "                                             pretrained=True, ctx=ctx, use_pooler=True,\n",
        "                                             use_decoder=False, use_classifier=False,\n",
        "                                             root='/content/drive/MyDrive/bert-mx')\n",
        "print(bert_base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Qbm9ijQLj-dz"
      },
      "outputs": [],
      "source": [
        "bert_classifier = nlp.model.BERTClassifier(bert_base, num_classes=2, dropout=0.1)\n",
        "# only need to initialize the classifier layer.\n",
        "bert_classifier.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
        "bert_classifier.hybridize(static_alloc=True)\n",
        "\n",
        "# softmax cross entropy loss for classification\n",
        "loss_function = gluon.loss.SoftmaxCELoss()\n",
        "loss_function.hybridize(static_alloc=True)\n",
        "\n",
        "metric = mx.metric.Accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "vPPde7qvj-dz"
      },
      "outputs": [],
      "source": [
        "tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "dTIEN5K-j-d0"
      },
      "outputs": [],
      "source": [
        "train_df['prompt'] = train_df['prompt'].astype(str)\n",
        "train_df['next'] = train_df['next'].astype(str)\n",
        "train_df['target']=train_df['target'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "AzM6inNXj-d0"
      },
      "outputs": [],
      "source": [
        "train_data_raw = train_df[['prompt', 'next', 'target']].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWEG1TJAj-d0",
        "outputId": "66aaad0b-a53a-4514-d14a-5441a39d696a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gluonnlp/data/batchify/batchify.py:235: UserWarning: Padding value is not given and will be set automatically to 0 in data.batchify.Pad(). Please check whether this is intended (e.g. value of padding index in the vocabulary).\n",
            "  'Padding value is not given and will be set automatically to 0 '\n"
          ]
        }
      ],
      "source": [
        "pool = multiprocessing.Pool()\n",
        "\n",
        "# transformation for data train and dev\n",
        "#label_dtype = 'float32' if not task.class_labels else 'int32'\n",
        "label_dtype='int32'\n",
        "trans = BERTDatasetTransform(tokenizer, max_len,\n",
        "                             class_labels=class_labels,\n",
        "                             pad=pad, pair=pair,\n",
        "                             has_label=True)\n",
        "\n",
        "# data train\n",
        "# task.dataset_train returns (segment_name, dataset)\n",
        "#train_tsv = task.dataset_train()[1]\n",
        "#data_train = mx.gluon.data.SimpleDataset(pool.map(trans, train_data_raw))\n",
        "data_train = mx.gluon.data.SimpleDataset(train_data_raw)\n",
        "data_train = data_train.transform(trans)\n",
        "data_train_len = data_train.transform(\n",
        "    lambda input_id, length, segment_id, label_id: length, lazy=False)\n",
        "# bucket sampler for training\n",
        "batchify_fn = nlp.data.batchify.Tuple(\n",
        "    nlp.data.batchify.Pad(axis=0), nlp.data.batchify.Stack(),\n",
        "    nlp.data.batchify.Pad(axis=0), nlp.data.batchify.Stack(label_dtype))\n",
        "batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
        "    data_train_len,\n",
        "    batch_size=batch_size,\n",
        "    #num_buckets=10,\n",
        "    ratio=0,\n",
        "    shuffle=True)\n",
        "# data loader for training\n",
        "train_data = gluon.data.DataLoader(\n",
        "    dataset=data_train,\n",
        "    num_workers=4,\n",
        "    batch_sampler=batch_sampler,\n",
        "    batchify_fn=batchify_fn)\n",
        "num_train_examples = len(data_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6p-9PWjj-d1",
        "outputId": "9a0388b4-f4ac-43b4-fced-24d7e635348c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary used for tokenization = \n",
            "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n",
            "[PAD] token id = 1\n",
            "[CLS] token id = 2\n",
            "[SEP] token id = 3\n",
            "token ids = \n",
            "[ 2  0  0  0  0  0  0  0  0  0  0  0  0  0 54  3  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0 54  3  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  1  1  1  1  1]\n",
            "valid length = \n",
            "32\n",
            "segment ids = \n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "label = \n",
            "[1]\n"
          ]
        }
      ],
      "source": [
        "print('vocabulary used for tokenization = \\n%s'%vocabulary)\n",
        "print('[PAD] token id = %s'%(vocabulary['[PAD]']))\n",
        "print('[CLS] token id = %s'%(vocabulary['[CLS]']))\n",
        "print('[SEP] token id = %s'%(vocabulary['[SEP]']))\n",
        "print('token ids = \\n%s'%data_train[4][0])\n",
        "print('valid length = \\n%s'%data_train[4][1])\n",
        "print('segment ids = \\n%s'%data_train[4][2])\n",
        "print('label = \\n%s'%data_train[4][3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KObIxzsj-d2"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "NFEC43L3j-d2"
      },
      "outputs": [],
      "source": [
        "optimizer = mx.optimizer.create('adam', learning_rate=lr, epsilon=epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "FpWoUn4Tj-d3"
      },
      "outputs": [],
      "source": [
        "all_model_params = bert_classifier.collect_params()\n",
        "optimizer_params = {'learning_rate': lr, 'epsilon': epsilon, 'wd': 0.01}\n",
        "\n",
        "try:\n",
        "    trainer = gluon.Trainer(all_model_params, optimizer,\n",
        "                            update_on_kvstore=False)\n",
        "except ValueError as e:\n",
        "    print(e)\n",
        "    warnings.warn(\n",
        "        'AdamW optimizer is not found. Please consider upgrading to '\n",
        "        'mxnet>=1.5.0. Now the original Adam optimizer is used instead.')\n",
        "    trainer = gluon.Trainer(all_model_params, 'adam',\n",
        "                            optimizer_params, update_on_kvstore=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bert_classifier.load_parameters('/content/drive/MyDrive/bert-mx/j-20190616k', ctx=ctx)"
      ],
      "metadata": {
        "id": "V4NKKN5E_TeV"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb72-Xxrj-d5",
        "outputId": "b4681c8d-42d5-48d7-a66e-43ad830d820c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0 Batch 100/474] loss=0.0697, lr=0.0000007, acc=0.498\n",
            "[Epoch 0 Batch 200/474] loss=0.0690, lr=0.0000010, acc=0.521\n",
            "[Epoch 0 Batch 300/474] loss=0.0686, lr=0.0000009, acc=0.533\n",
            "[Epoch 0 Batch 400/474] loss=0.0689, lr=0.0000008, acc=0.538\n",
            "[Epoch 1 Batch 100/474] loss=0.0683, lr=0.0000007, acc=0.569\n",
            "[Epoch 1 Batch 200/474] loss=0.0686, lr=0.0000006, acc=0.559\n",
            "[Epoch 1 Batch 300/474] loss=0.0683, lr=0.0000005, acc=0.561\n",
            "[Epoch 1 Batch 400/474] loss=0.0683, lr=0.0000004, acc=0.560\n",
            "[Epoch 2 Batch 100/474] loss=0.0680, lr=0.0000003, acc=0.565\n",
            "[Epoch 2 Batch 200/474] loss=0.0680, lr=0.0000002, acc=0.569\n",
            "[Epoch 2 Batch 300/474] loss=0.0679, lr=0.0000001, acc=0.569\n",
            "[Epoch 2 Batch 400/474] loss=0.0680, lr=0.0000000, acc=0.567\n"
          ]
        }
      ],
      "source": [
        "step_size = batch_size * accumulate if accumulate else batch_size\n",
        "num_train_steps = int(num_train_examples / step_size * epochs)\n",
        "num_warmup_steps = int(num_train_steps * warmup_ratio)\n",
        "step_num = 0\n",
        "\n",
        "# Do not apply weight decay on LayerNorm and bias terms\n",
        "for _, v in bert_classifier.collect_params('.*beta|.*gamma|.*bias').items():\n",
        "    v.wd_mult = 0.0\n",
        "# Collect differentiable parameters\n",
        "params = [p for p in all_model_params.values() if p.grad_req != 'null']\n",
        "\n",
        "# Set grad_req if gradient accumulation is required\n",
        "if accumulate:\n",
        "    for p in params:\n",
        "        p.grad_req = 'add'\n",
        "\n",
        "#tic = time.time()\n",
        "for epoch_id in range(epochs):\n",
        "    metric.reset()\n",
        "    step_loss = 0\n",
        "    #tic = time.time()\n",
        "    all_model_params.zero_grad()\n",
        "\n",
        "    for batch_id, seqs in enumerate(train_data):\n",
        "        # learning rate schedule\n",
        "        if step_num < num_warmup_steps:\n",
        "            new_lr = lr * step_num / num_warmup_steps\n",
        "        else:\n",
        "            non_warmup_steps = step_num - num_warmup_steps\n",
        "            offset = non_warmup_steps / (num_train_steps - num_warmup_steps)\n",
        "            new_lr = lr - offset * lr\n",
        "        optimizer.set_learning_rate(new_lr)\n",
        "\n",
        "        # forward and backward\n",
        "        with mx.autograd.record():\n",
        "            input_ids, valid_length, type_ids, label = seqs\n",
        "            out = bert_classifier(\n",
        "                input_ids.as_in_context(ctx), type_ids.as_in_context(ctx),\n",
        "                valid_length.astype('float32').as_in_context(ctx))\n",
        "            ls = loss_function(out, label.as_in_context(ctx)).mean()\n",
        "        ls.backward()\n",
        "\n",
        "        # update\n",
        "        if not accumulate or (batch_id + 1) % accumulate == 0:\n",
        "            trainer.allreduce_grads()\n",
        "            nlp.utils.clip_grad_global_norm(params, 1)\n",
        "            trainer.update(accumulate if accumulate else 1)\n",
        "            # set grad to zero for gradient accumulation\n",
        "            all_model_params.zero_grad()\n",
        "            step_num += 1\n",
        "\n",
        "        step_loss += ls.asscalar()\n",
        "        metric.update([label], [out])\n",
        "        if (batch_id + 1) % (100) == 0:\n",
        "            #log_train(batch_id, len(train_data), metric, step_loss, log_interval,\n",
        "                      #epoch_id, trainer.learning_rate)\n",
        "            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}'\n",
        "                  .format(epoch_id, batch_id + 1, len(train_data),\n",
        "                 step_loss / log_interval,\n",
        "                 optimizer.learning_rate, metric.get()[1]))\n",
        "            step_loss = 0\n",
        "    mx.nd.waitall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FGgSJRsoj-d6"
      },
      "outputs": [],
      "source": [
        "bert_classifier.save_parameters('/content/drive/MyDrive/bert-mx/j-20220326k')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The hyperparameters\n",
        "batch_size = 32\n",
        "lr = 5e-6\n",
        "\n",
        "# The FixedBucketSampler and the DataLoader for making the mini-batches\n",
        "train_sampler = nlp.data.FixedBucketSampler(lengths=[int(item[1]) for item in data_train],\n",
        "                                            batch_size=batch_size,\n",
        "                                            shuffle=True)\n",
        "bert_dataloader = mx.gluon.data.DataLoader(data_train, batch_sampler=train_sampler)\n",
        "\n",
        "trainer = mx.gluon.Trainer(bert_classifier.collect_params(), 'adam',\n",
        "                           {'learning_rate': lr, 'epsilon': 1e-9})\n",
        "\n",
        "# Collect all differentiable parameters\n",
        "# `grad_req == 'null'` indicates no gradients are calculated (e.g. constant parameters)\n",
        "# The gradients for these params are clipped later\n",
        "params = [p for p in bert_classifier.collect_params().values() if p.grad_req != 'null']\n",
        "grad_clip = 1\n",
        "\n",
        "# Training the model with only three epochs\n",
        "log_interval = 4\n",
        "num_epochs = 3\n",
        "for epoch_id in range(num_epochs):\n",
        "    metric.reset()\n",
        "    step_loss = 0\n",
        "    for batch_id, (token_ids, segment_ids, valid_length, label) in enumerate(bert_dataloader):\n",
        "        with mx.autograd.record():\n",
        "\n",
        "            # Load the data to the GPU\n",
        "            token_ids = token_ids.as_in_context(ctx)\n",
        "            valid_length = valid_length.as_in_context(ctx)\n",
        "            segment_ids = segment_ids.as_in_context(ctx)\n",
        "            label = label.as_in_context(ctx)\n",
        "\n",
        "            # Forward computation\n",
        "            out = bert_classifier(token_ids, segment_ids, valid_length.astype('float32'))\n",
        "            ls = loss_function(out, label).mean()\n",
        "\n",
        "        # And backwards computation\n",
        "        ls.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        trainer.allreduce_grads()\n",
        "        nlp.utils.clip_grad_global_norm(params, 1)\n",
        "        trainer.update(1)\n",
        "\n",
        "        step_loss += ls.asscalar()\n",
        "        metric.update([label], [out])\n",
        "\n",
        "        # Printing vital information\n",
        "        if (batch_id + 1) % (log_interval) == 0:\n",
        "            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}'\n",
        "                         .format(epoch_id, batch_id + 1, len(bert_dataloader),\n",
        "                                 step_loss / log_interval,\n",
        "                                 trainer.learning_rate, metric.get()[1]))\n",
        "            step_loss = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "zSumKpNccOSa",
        "outputId": "fe5529d9-0329-4410-81e3-624a4802aeb2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MXNetError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-63fe79ec5bcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# Forward computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gluonnlp/model/bert.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, token_types, valid_length)\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0mShape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \"\"\"\n\u001b[0;32m--> 866\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBERTClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhybrid_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mxnet/gluon/block.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mxnet/gluon/block.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m   1490\u001b[0m                                      'Find all contexts = {}'.format(ctx_set))\n\u001b[1;32m   1491\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_cached_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mxnet/gluon/block.py\u001b[0m in \u001b[0;36m_call_cached_op\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         cargs = [args_without_none[i] if is_arg else i.data()\n\u001b[1;32m   1232\u001b[0m                  for is_arg, i in self._cached_op_args]\n\u001b[0;32m-> 1233\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mxnet/_ctypes/ndarray.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             ctypes.byref(out_stypes)))\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moriginal_output\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mxnet/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \"\"\"\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mget_last_ffi_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMXNetError\u001b[0m: MXNetError: Error in operator bertmodel1__plus0: [13:36:40] ../src/ndarray/./../operator/tensor/../elemwise_op_common.h:134: Check failed: assign(&dattr, vec.at(i)): Incompatible attr in node bertmodel1__plus0 at 1-th input: expected [32,128,768], got [32,768]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": false,
        "id": "XUNlxxNqj-d7"
      },
      "source": [
        "### prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Ud1UYCNij-d7"
      },
      "outputs": [],
      "source": [
        "test_df['prompt'] = test_df['prompt'].astype(str)\n",
        "test_df['next'] = test_df['next'].astype(str)\n",
        "test_data_raw = test_df[['prompt', 'next']].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAfDj8fbj-d7",
        "outputId": "27d54da3-c324-4fe0-fdb6-19065defa0e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gluonnlp/data/batchify/batchify.py:235: UserWarning: Padding value is not given and will be set automatically to 0 in data.batchify.Pad(). Please check whether this is intended (e.g. value of padding index in the vocabulary).\n",
            "  'Padding value is not given and will be set automatically to 0 '\n"
          ]
        }
      ],
      "source": [
        "# batchify for data test\n",
        "test_batchify_fn = nlp.data.batchify.Tuple(\n",
        "    nlp.data.batchify.Pad(axis=0), nlp.data.batchify.Stack(),\n",
        "    nlp.data.batchify.Pad(axis=0))\n",
        "\n",
        "# transform for data test\n",
        "test_trans = BERTDatasetTransform(tokenizer, max_len,\n",
        "                                  class_labels=None,\n",
        "                                  pad=pad, pair=pair,\n",
        "                                  has_label=False)\n",
        "\n",
        "data_test = mx.gluon.data.SimpleDataset(pool.map(test_trans, test_data_raw))\n",
        "loader_test = mx.gluon.data.DataLoader(\n",
        "    data_test,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4,\n",
        "    shuffle=False,\n",
        "    batchify_fn=test_batchify_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgMCF1Tdj-d7",
        "outputId": "bdcd86ee-132d-4a85-fb4a-a6ccecae2ad3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 2,  0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0, 54,  3,  1,  1,  1,\n",
              "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "         1,  1,  1,  1,  1,  1,  1,  1,  1], dtype=int32),\n",
              " array(14, dtype=int32),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "data_test[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "81Te531tj-d8"
      },
      "outputs": [],
      "source": [
        "#value_list = []\n",
        "#index_list = []\n",
        "results = []\n",
        "for _, seqs in enumerate(loader_test):\n",
        "    input_ids, valid_length, type_ids = seqs\n",
        "    out = bert_classifier(input_ids.as_in_context(ctx),\n",
        "                type_ids.as_in_context(ctx),\n",
        "                valid_length.astype('float32').as_in_context(ctx))\n",
        "    results.extend([o for o in out.asnumpy()])\n",
        "    #values, indices = mx.nd.topk(out, k=1, ret_typ='both')\n",
        "    #value_list.extend(values.asnumpy().reshape(-1).tolist())\n",
        "    #index_list.extend(indices.asnumpy().reshape(-1).tolist())\n",
        "\n",
        "mx.nd.waitall()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STUsbiDCj-d8",
        "outputId": "c4fa967c-4751-4719-d983-2747846746d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.02147342, -0.07293352], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "results[24]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "J9lEZ5SYj-d8"
      },
      "outputs": [],
      "source": [
        "predictions = [mx.nd.array(result).sigmoid().asnumpy()[1] for result in results]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "IzXSStb4j-d8"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame.from_dict({\n",
        "    'prompt': test_df['prompt'],\n",
        "    'next': test_df['next'],\n",
        "    'prediction': predictions\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "DqH_tDkUj-d9"
      },
      "outputs": [],
      "source": [
        "submission.to_csv('/content/drive/MyDrive/data/submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1kibzKV95EF4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "fine-tuning-bert-with-mxnet-gluon.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}